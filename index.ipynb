{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import streamlit as st\n",
    "# pip install tokenizers datasets torch streamlit pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('hospital_support.csv', sep=';', skiprows=1)\n",
    "\n",
    "def preprocess(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['question'] = df['question'].apply(preprocess)\n",
    "df['answer'] = df['answer'].apply(preprocess)\n",
    "\n",
    "# Prepare texts for training the tokenizer\n",
    "texts = df['question'].tolist() + df['answer'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train custom tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=[\n",
    "                              \"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"])\n",
    "tokenizer.train_from_iterator(texts, trainer)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    pair=\"<s> $A </s> <s> $B </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    ],\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=128)  # Reduced max_length for memory efficiency\n",
    "tokenizer.enable_padding(\n",
    "    pad_id=tokenizer.token_to_id(\"<pad>\"), pad_token=\"<pad>\")\n",
    "\n",
    "# Save and load tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n",
    "tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a simple sequence-to-sequence model using LSTM\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout=0.5):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                               n_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim,\n",
    "                               n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        embedded_src = self.embedding(src)\n",
    "        embedded_trg = self.embedding(trg)\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "        outputs, _ = self.decoder(embedded_trg, (hidden, cell))\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model parameters\n",
    "input_dim = tokenizer.get_vocab_size()\n",
    "embedding_dim = 256  # Dimension of the embeddings\n",
    "hidden_dim = 256\n",
    "output_dim = tokenizer.get_vocab_size()\n",
    "n_layers = 2\n",
    "model = Seq2SeqModel(input_dim, embedding_dim, hidden_dim, output_dim, n_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = [ex for ex in examples['question']]\n",
    "    targets = [ex for ex in examples['answer']]\n",
    "    model_inputs = tokenizer.encode_batch(inputs)\n",
    "    labels = tokenizer.encode_batch(targets)\n",
    "    model_inputs = {'input_ids': [x.ids for x in model_inputs]}\n",
    "    labels = {'labels': [x.ids for x in labels]}\n",
    "    return {**model_inputs, **labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert dataframe to Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"question\", \"answer\"])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoader for batching\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids'], dtype=torch.long) for item in batch]\n",
    "    labels = [torch.tensor(item['labels'], dtype=torch.long) for item in batch]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.token_to_id('<pad>'))\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.token_to_id('<pad>'))\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "def train(model, train_dataloader, optimizer, criterion, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            src = batch['input_ids']\n",
    "            trg = batch['labels']\n",
    "            output = model(src, trg)\n",
    "            loss = criterion(output.view(-1, output_dim), trg.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_dataloader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('<pad>'))\n",
    "\n",
    "# Train the model\n",
    "train(model, train_dataloader, optimizer, criterion, epochs=5)  # Increased epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate chatbot responses\n",
    "def chatbot_response(question):\n",
    "    inputs = tokenizer.encode(question).ids\n",
    "    inputs = torch.tensor(inputs, dtype=torch.long).unsqueeze(0)\n",
    "    trg = torch.zeros((1, 128), dtype=torch.long).fill_(tokenizer.token_to_id('<pad>'))\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs, trg)\n",
    "    output_ids = output.argmax(-1).squeeze().tolist()\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Streamlit interface\n",
    "st.title(\"Hospital Support Chatbot\")\n",
    "st.write(\"Welcome to the Hospital Support Chatbot. Please type a message and press Enter to start the conversation.\")\n",
    "\n",
    "user_input = st.text_input(\"You:\")\n",
    "if user_input:\n",
    "    response = chatbot_response(user_input)\n",
    "    st.text_area(\"Chatbot:\", value=response, height=100, max_chars=None)\n",
    "    if response.lower() in ['goodbye', 'bye']:\n",
    "        st.write(\"Thank you for chatting with me. Have a great day!\")\n",
    "        st.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
